x-common-env: &common-env
  KAFKA_CONTROLLER_LISTENER_NAMES: 'CONTROLLER'
  KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'CONTROLLER:PLAINTEXT,EXTERNAL:PLAINTEXT,INTERNAL:PLAINTEXT'
  KAFKA_INTER_BROKER_LISTENER_NAME: 'INTERNAL'
  KAFKA_CONTROLLER_QUORUM_VOTERS: '1@broker1:29093,2@broker2:29093' # 2 brokers in the quorum
  KAFKA_PROCESS_ROLES: 'broker,controller'
  KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
  KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 2
  KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 2
  CLUSTER_ID: 'a2Fma2EtY2x1c3Rlcg==' #kafka-cluster base64 encoded string
  KAFKA_LOG_DIRS: /tmp/kafka-combined-logs
  KAFKA_METRIC_REPORTERS: io.confluent.metrics.reporter.ConfluentMetricsReporter
  CONFLUENT_METRICS_REPORTER_TOPIC_REPLICAS: 1
  CONFLUENT_METRICS_ENABLE: 'true'
  CONFLUENT_SUPPORT_CUSTOMER_ID: anonymous

x-common-healthcheck: &common-healthcheck
  interval: 10s
  timeout: 5s
  retries: 5

services:
  broker1:
    image: confluentinc/cp-server
    container_name: broker1
    hostname: broker1
    ports:
      - "9092:9092"
    environment:
      <<: *common-env
      KAFKA_NODE_ID : 1
      KAFKA_LISTENERS: 'INTERNAL://broker1:29092,CONTROLLER://broker1:29093,EXTERNAL://0.0.0.0:9092'
      KAFKA_ADVERTISED_LISTENERS: 'INTERNAL://broker1:29092,EXTERNAL://localhost:9092'
      CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS: 'broker1:29092'
    healthcheck:
      <<: *common-healthcheck
      test: ["CMD", "bash", "-c", "echo 'ok' | nc localhost 9092"]
    networks:
      - data_streaming

  broker2:
    image: confluentinc/cp-server
    container_name: broker2
    hostname: broker2
    ports:
      - "9093:9093"
    environment:
      <<: *common-env
      KAFKA_NODE_ID : 2
      KAFKA_LISTENERS: 'INTERNAL://broker2:29092,CONTROLLER://broker2:29093,EXTERNAL://0.0.0.0:9093'
      KAFKA_ADVERTISED_LISTENERS: 'INTERNAL://broker2:29092,EXTERNAL://localhost:9093'
      CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS: 'broker2:29092'
    healthcheck:
      <<: *common-healthcheck
      test: ["CMD", "bash", "-c", "echo 'ok' | nc localhost 9093"]
    networks:
      - data_streaming

  control-center:
    image: confluentinc/cp-enterprise-control-center
    container_name: control-center
    hostname: control-center
    # depends_on:
    #   schema-registry:
    #     condition: service_healthy
    ports:
      - "9021:9021"
    environment:
        CONTROL_CENTER_BOOTSTRAP_SERVERS: 'broker1:29092,broker2:29092'
        # CONTROL_CENTER_SCHEMA_REGISTRY_URL: 'http://schema-registry:8081'
        CONTROL_CENTER_REPLICATION_FACTOR: 1
        CONTROL_CENTER_INTERNAL_TOPICS_PARTITIONS: 1
        CONTROL_CENTER_MONITORING_INTERCEPTOR_TOPIC_PARTITIONS: 1
        CONFLUENT_METRICS_TOPIC_REPLICATION: 1
        CONFLUENT_METRICS_ENABLE: 'false'
        PORT: 9021
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9021/"]
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
      - data_streaming

  minio:
    image: minio/minio
    container_name: minio
    hostname: minio
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    command: server /data --console-address ":9001"
    networks:
      - data_streaming

  mc-init:
    image: 'minio/mc'  # only run once to add bucket
    container_name: mc-init
    tty: true
    entrypoint: |
      /bin/bash -c "
      sleep 5;
      /usr/bin/mc config --quiet host add dmlminio http://minio:9000 minioadmin minioadmin;
      /usr/bin/mc mb dmlminio/bronze;
      /usr/bin/mc admin service restart dmlminio;
      "
    depends_on:
      - minio
    networks:
      - data_streaming

  spark-master:
    image: bitnami/spark:latest
    command: bin/spark-class org.apache.spark.deploy.master.Master
    volumes:
      - ./jobs:/opt/bitnami/spark/jobs
    ports:
      - "9090:8080"
      - "7077:7077"
    networks:
      - data_streaming

  spark-worker:
    image: bitnami/spark:latest
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    depends_on:
      - spark-master
    volumes:
      - ./jobs:/opt/bitnami/spark/jogs
    environment:
      SPARK_MODE: worker
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: 1g
      SPARK_MASTER_URL: spark://spark-master:7077
    networks:
      - data_streaming

networks:
  data_streaming:
